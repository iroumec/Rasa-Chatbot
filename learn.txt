---------------------------------------------------------------------------------------------------
# Dejando un espacio, Telegram los envía como dos mensajes por separado.

responses:
  utter_cheer_up:
  - text: |
      Lamento oír eso. Creo que este video podría animarte:
      
      https://youtu.be/tpiyEe_CqB4
---------------------------------------------------------------------------------------------------
## A una regla le podés colocar una condición de que solo se ejecute cuando es el comienzo de la
# conversación:

- rule: Darle la bienvenida al usuario
  conversation_start: true # De esta forma, la regla no solo aplica al principio de la conversación
  steps:
    - intent: greet
    - action: utter_welcome
---------------------------------------------------------------------------------------------------
# Que el bot de diferentes respuestas dependiendo de si un slot está lleno o no

https://forum.rasa.com/t/different-responses-based-on-a-slot-is-given-or-not/47513
---------------------------------------------------------------------------------------------------
# Historias condicionales en rasa

https://forum.rasa.com/t/how-can-i-write-conditional-story-or-rules/47049
---------------------------------------------------------------------------------------------------
# Conectar la API de YouTube a Python

https://developers.google.com/youtube/v3/quickstart/python?hl=es-419

https://youtu.be/1ifgZ9Z1BIc?feature=shared # Video tutorial
---------------------------------------------------------------------------------------------------
# Lista de reproducción con tutoriales de cómo buscar videos en YouTube

https://youtube.com/playlist?list=PLycDrvqd1Sop7Rz5O92w2yJIo-IkChmWQ&feature=shared

pip install panda
pip install apiclient
pip install oauth2client
pip install httplib2
pip install google-api-python-client
pip install google-auth-httplib2
pip install google-auth-oauthlib

https://github.com/jgvasque93/bot-ypoutube
---------------------------------------------------------------------------------------------------
# Instalar una librería en un entorno específico de Conda

  0. Entrar a Conda

  1. Elegir el environment donde querés instalar el paquete

  2. Entrar a CMD.exe Prompt y asegurarse de que, entre paréntesis, diga el nombre del environment que utilizás

  3. Instalar las librerías
---------------------------------------------------------------------------------------------------
Si se descomenta endpoint, en una consola aparte se debe ejecutar "rasa run actions"

HACE LITERALMENTE DÍAS QUE TENGO PROBLEMAS POR ESA ESTUPIDEZ
---------------------------------------------------------------------------------------------------
El error PermissionDenied [Windows 5] parece ocurrir cuando se hace una copia de seguridad en Drive
de la carpeta donde está el chatbot.

Para solucionarlo, desactivar la copia de seguridad de la carpeta, borrar todos los modelos entre-
nados y eliminar todos los archivos temporales.

------------------------------------------------------------------------------------------------

- slot_was_set:
  - topic: example # No nos importa el valor, puesto a que "influence_conversation: false" en el slot.

------------------------------------------------------------------------------------------------
switch vs if en Python:
https://ellibrodepython.com/switch-python

---------------------------------------------------------------------------------------------------

Se instalo pip install pytube

What is BeautifulSoup library and How to install it: https://www.projectpro.io/recipes/what-is-beautifulsoup-library-and-install-it#:~:text=Beautiful%20Soup%20(bs4)%20is%20a,Last%20Updated%3A%2008%20Aug%202022

/////////////////////

Orden para iniciar el bot: rasa run actions, ngrok, rasa run.

---------------------------------------------------------

En Rasa la condición para un texto puede ir arriba o bajo. Estas son las dos formas de escribirlo:

- condition:
    ...
  text: example test

- text: example test
  condition:
    ...

-----------------------------------------------------------------------------------------------------------------------

Expresión regular para reconocer nombres:

/^[a-z ,.'-]+$/i

Expresión regular para reconocer nombres con caracteres especiales (support international names with super sweet unicode):

/^[a-zA-ZàáâäãåąčćęèéêëėįìíîïłńòóôöõøùúûüųūÿýżźñçčšžÀÁÂÄÃÅĄĆČĖĘÈÉÊËÌÍÎÏĮŁŃÒÓÔÖÕØÙÚÛÜŲŪŸÝŻŹÑßÇŒÆČŠŽ∂ð ,.'-]+$/u

Sin el circunflexo para que no tengas que estar sí o sí al comienzo:

/[a-zA-ZàáâäãåąčćęèéêëėįìíîïłńòóôöõøùúûüųūÿýżźñçčšžÀÁÂÄÃÅĄĆČĖĘÈÉÊËÌÍÎÏĮŁŃÒÓÔÖÕØÙÚÛÜŲŪŸÝŻŹÑßÇŒÆČŠŽ∂ð ,.'-]+/u

Videos about Regular Expressions:

https://www.youtube.com/watch?v=jCE9qHkSyoE

-----------------------------------------------------------------------------------------------------------------------

Swiplserver es solo una herramienta para trabajar con Prolog dentro de Python. Pero hay otras igual de útiles como lo
es Pyswip. Podemos utilizar la que queramos y nos resulte más cómoda

-----------------------------------------------------------------------------------------------------------------------

Al momento de buscar el topic, la temática, se plantearon estas dos alternativas:

# Elimino las palabras iniciales que solemos usar como introductorias.
# Recordar que "replace()" no remplaza sobre la variable, sino que devuelve una copia remplazada.
if (message.startswith('de ')):
    message = message.replace('de ', '')
elif (message.startswith('sobre ')):
    message = message.replace('sobre ', '')
elif (message.startswith('acerca de ')):
    message = message.replace('acerca de ', '')
elif (message.startswith('del ')):
    message = message.replace('del ', '')
            
# Separo el mensaje de las palabras que solemos usar como introductorias a la temática.
# Divido el mensaje en dos partes: la que va al comienzo y termina en la sentencia especificada, y la parte
# que comienza luego de esa palabra introductoria, que será la temática.
if 'de ' in message:
    splitMessage = message.split('de ')
    message = splitMessage[1]
elif 'sobre ' in message:
    splitMessage = message.split('sobre ')
    message = splitMessage[1]
elif 'acerca de ' in message:
    splitMessage = message.split('acerca de ')
    message = splitMessage[1]
elif 'del ' in message:
    splitMessage = message.split('del ')
    message = splitMessage[1]

Finalmente, se optó por la segunda, puesto a que la primera no presentaba escalabilidad. Si yo le agregaba más cosas al
mensaje al comienzo, no iba a reconocerme la temática.

-----------------------------------------------------------------------------------------------------------------------

displaCy Named Entity Visualizer

https://demos.explosion.ai/displacy-ent?text=I%C3%B1aki%20fue%20a%20la%20tienda%20de%20Juan%20a%20comprarle%20flores%20a%20Clara%20Peterson&model=es_core_news_sm&ents=person%2Cper

Para probar cómo funcionaría el componente SpacyEntityExtractor

Para utilizar Spacy:

pip install rasa[spacy]
python -m spacy download es_core_news_sm

La última línea es para instalar el modelo en español

In Rasa, `WhiteSpaceTokenizer` and `SpacyTokenizer` are two different tokenizers that you can use in your NLU (Natural Language Understanding) pipeline to break text into individual words or tokens. Each has its own characteristics and use cases. Here's a comparison of `WhiteSpaceTokenizer` and `SpacyTokenizer`:

1. **WhiteSpaceTokenizer**:
   - **Basic Tokenization**: `WhiteSpaceTokenizer` is a simple tokenizer that splits text into tokens based on white spaces (spaces, tabs, newlines, etc.). It doesn't perform more advanced linguistic analysis.
   - **Speed**: It's faster than `SpacyTokenizer` because it doesn't involve the complex linguistic analysis that Spacy performs.
   - **Suitability**: It's suitable for languages and use cases where simple whitespace-based tokenization is sufficient, such as English text. It may not be ideal for languages with complex morphology or languages with no clear word delimiters.
   - **Customization**: Limited customization options for tokenization rules.

   Example configuration in `config.yml`:
   ```yaml
   pipeline:
     - name: "WhitespaceTokenizer"
   ```

2. **SpacyTokenizer**:
   - **Linguistic Analysis**: `SpacyTokenizer` uses the Spacy library, which provides advanced linguistic analysis. It can recognize word boundaries, punctuation, and various linguistic features of the text.
   - **Language Support**: Spacy supports multiple languages and can handle complex languages with ease. You can load language-specific Spacy models.
   - **Customization**: Spacy allows for more fine-grained customization and rule-based tokenization, making it suitable for complex language processing tasks.
   - **Accuracy**: Spacy provides more accurate tokenization, especially for languages with complex morphology or non-trivial tokenization rules.

   Example configuration in `config.yml`:
   ```yaml
   pipeline:
     - name: "SpacyTokenizer"
   ```

-----------------------------------------------------------------------------------------------------------------------

WhitespaceTokenizer vs. SpacyTokenizer

WhiteSpaceTokenizer:
- Basic Tokenization: WhiteSpaceTokenizer is a simple tokenizer that splits text into tokens based on white spaces
(spaces, tabs, newlines, etc.). It doesn't perform more advanced linguistic analysis.
- Speed: It's faster than SpacyTokenizer because it doesn't involve the complex linguistic analysis that Spacy performs.
- Suitability: It's suitable for languages and use cases where simple whitespace-based tokenization is sufficient, 
such as English text. It may not be ideal for languages with complex morphology or languages with no clear word delimiters.
- Customization: Limited customization options for tokenization rules.

SpacyTokenizer:
- Linguistic Analysis: SpacyTokenizer uses the Spacy library, which provides advanced linguistic analysis. It can 
recognize word boundaries, punctuation, and various linguistic features of the text.
- Language Support: Spacy supports multiple languages and can handle complex languages with ease. You can load 
language-specific Spacy models.
- Customization: Spacy allows for more fine-grained customization and rule-based tokenization, making it suitable for 
complex language processing tasks.
- Accuracy: Spacy provides more accurate tokenization, especially for languages with complex morphology or non-trivial 
tokenization rules.

In summary, `WhiteSpaceTokenizer` is a lightweight tokenizer suitable for simple tokenization tasks, while
`SpacyTokenizer` provides advanced linguistic analysis, making it suitable for languages with complex grammar 
and morphology. The choice between them depends on your language, use case, and the level of linguistic analysis 
you need for your chatbot or NLU model. If you're working with English or another language well-supported by Spacy, 
it's often a good choice for better accuracy and language understanding.

-----------------------------------------------------------------------------------------------------------------------

Datos curiosos sobre Spacy:
- Ni en en_core_web_sm ni en es_core_news_sm se reconoce el nombre Guadalupe.
- Iñaki en en_core_web_sm se lo considera como la etiqueta GPE.
- sm -> small model size, md -> medium model size, lg -> large (or regular) model size.
- En es_core_news_md a Guadalupe lo toma con la etiqueta LOC (location).
- Cristobal tampoco es reconocido en es_core_news_md.
- En el modelo es_core_news_lg sí aparece Cristobal, pero aún no figura Guadalupe.
- En las versiones más nuevas de Spacy, la etiqueta PERSON ahora es PER.
- Si se desea reconocer el nombre Guadalupe, que es tomado como LOC, al igual que otros nombres que corresponden
  a lugares, debemos hacer un chequeo extra.

NO USAR EL MODELO LARGE. PROBLEMA TRAS PROBLEMA.

Si agrego "Hiiii", con muchas i, Spacy me lo toma como PER.

Gabriela y Graciela no se encuentran en la base de datos de Spacy

-----------------------------------------------------------------------------------------------------------------------

Assuming you are working on your own project, or are a technical lead on a project: You should always use CamelCase
for class names and snake_case for everything else in Python as recommended in PEP8.

-----------------------------------------------------------------------------------------------------------------------

# EmbeddingIntentClassifier me permite identificar intents múltiples.
# Más info: https://rasa.com/blog/how-to-handle-multiple-intents-per-input-using-rasa-nlu-tensorflow-pipeline/
# Ahora se llama DIETClasifier: https://forum.rasa.com/t/error-in-using-embeddingintentclassifier-for-multiple-intent-case/49772

-----------------------------------------------------------------------------------------------------------------------

En un principio, debido a que el slot "format" únicamente almacena tres valores: video, channel y playlist, quise
proponer sinónimos para los mismos.

- synonym: video
  examples: |
    - videardo
    - videoardo
    - vídeo
    - videiño
    - videinio
    - video de youtube
    - videos
    - videoo

- synonym: channel
  examples: |
    - canal
    - cuenta
    - usuario
    - canal de youtube

- synonym: playlist
  examples: |
    - lista de reproducción
    - lista
    - lista de videos
    - list of reproduction

Sin embargo, no es posible asignarle sinónimos a las entities y a las slots.

-----------------------------------------------------------------------------------------------------------------------

En [lista de videos]{"entity": "format", "value": "playlist"}, en Rasa Interactive, solo reconoce "[lista](format)".
Por eso se agrega [lista]{"entity": "format", "value": "playlist"}.

-----------------------------------------------------------------------------------------------------------------------

if tags:
    print(f'Tags del video ({video_id}): {", ".join(tags)}')
else:
    print(f'No se encontraron tags para el video ({video_id})')

-----------------------------------------------------------------------------------------------------------------------

El índice de Jaccard ( IJ ) o coeficiente de Jaccard ( IJ ) mide el grado de similitud entre dos conjuntos,
sea cual sea el tipo de elementos. Es decir, la cardinalidad de la intersección de ambos conjuntos dividida por
la cardinalidad de su unión.

def jaccard_similarity(list1, list2):
    set1 = set(list1)
    set2 = set(list2)
    intersection_size = len(set1.intersection(set2))
    union_size = len(set1.union(set2))
    similarity = intersection_size / union_size
    return similarity

def will_like_list(my_list, other_list, threshold=0.5):
    similarity = jaccard_similarity(my_list, other_list)
    print(similarity)
    if similarity >= threshold:
        return True
    else:
        return False

-----------------------------------------------------------------------------------------------------------------------

# Bibliotecas interesantes, pero que no se terminaron utilizando:

pip install swiplserver
pip install swiplserver pyswip

https://pypi.org/project/YoutubeTags/
pip install YoutubeTags

pip install tensorflow scikit-learn

-----------------------------------------------------------------------------------------------------------------------

NLP = Natural Lenguage Processing

-----------------------------------------------------------------------------------------------------------------------

# En 2023, los canales de YouTube se identifican por un usuario junto a un @
# Para buscar un canal por link, ya no se utiliza la ID, sino el nombre del canal sin espacios dentro del siguiente link:
# https://youtube.com/channel/{ID}

-----------------------------------------------------------------------------------------------------------------------

numeroVideo = random.randint(2, 9) # De 2 a 9 por cómo funcionan las listas

-----------------------------------------------------------------------------------------------------------------------

Este error se hizo varias veces presente:

Traceback (most recent call last):
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\site-packages\rasa\engine\graph.py", line 496, in __call__
    output = self._fn(self._component, **run_kwargs)
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\site-packages\rasa\core\policies\ted_policy.py", line 735, in train
    self.run_training(model_data, label_ids)
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\site-packages\rasa\core\policies\unexpected_intent_policy.py", line 492, in run_training
    self.compute_label_quantiles_post_training(model_data, label_ids)
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\site-packages\rasa\core\policies\unexpected_intent_policy.py", line 432, in compute_label_quantiles_post_training
    label_id_scores = self._collect_label_id_grouped_scores(
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\site-packages\rasa\core\policies\unexpected_intent_policy.py", line 804, in _collect_label_id_grouped_scores
    output_scores["similarities"][index, 0, candidate_label_id]
IndexError: index 256 is out of bounds for axis 0 with size 256

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "C:\Users\inaki\anaconda3\envs\Yriv\Scripts\rasa.exe\__main__.py", line 7, in <module>
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\site-packages\rasa\__main__.py", line 133, in main
    cmdline_arguments.func(cmdline_arguments)
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\site-packages\rasa\cli\train.py", line 61, in <lambda>
    train_parser.set_defaults(func=lambda args: run_training(args, can_exit=True))
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\site-packages\rasa\cli\train.py", line 101, in run_training
    training_result = train_all(
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\site-packages\rasa\api.py", line 105, in train
    return train(
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\site-packages\rasa\model_training.py", line 207, in train
    return _train_graph(
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\site-packages\rasa\model_training.py", line 286, in _train_graph
    trainer.train(
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\site-packages\rasa\engine\training\graph_trainer.py", line 105, in train
    graph_runner.run(inputs={PLACEHOLDER_IMPORTER: importer})
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\site-packages\rasa\engine\runner\dask.py", line 101, in run
    dask_result = dask.get(run_graph, run_targets)
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\site-packages\dask\local.py", line 557, in get_sync
    return get_async(
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\site-packages\dask\local.py", line 500, in get_async
    for key, res_info, failed in queue_get(queue).result():
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\concurrent\futures\_base.py", line 439, in result
    return self.__get_result()
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\concurrent\futures\_base.py", line 391, in __get_result
    raise self._exception
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\site-packages\dask\local.py", line 542, in submit
    fut.set_result(fn(*args, **kwargs))
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\site-packages\dask\local.py", line 238, in batch_execute_tasks
    return [execute_task(*a) for a in it]
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\site-packages\dask\local.py", line 238, in <listcomp>
    return [execute_task(*a) for a in it]
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\site-packages\dask\local.py", line 229, in execute_task
    result = pack_exception(e, dumps)
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\site-packages\dask\local.py", line 224, in execute_task
    result = _execute_task(task, data)
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\site-packages\dask\core.py", line 119, in _execute_task
    return func(*(_execute_task(a, cache) for a in args))
  File "C:\Users\inaki\anaconda3\envs\Yriv\lib\site-packages\rasa\engine\graph.py", line 503, in __call__
    raise GraphComponentException(
rasa.engine.exceptions.GraphComponentException: Error running graph component for node train_UnexpecTEDIntentPolicy2.

SOLUCIÓN: AGREGAR MÁS STORIES